version: "3.9"

services:
  llama:
    image: ghcr.io/ggerganov/llama.cpp:server
    command:
      - --model
      - /models/qwen2.5-1.5b-instruct.gguf
      - --host
      - 0.0.0.0
      - --port
      - "8080"
    volumes:
      - ./models:/models:ro
    ports:
      - "8080:8080"
